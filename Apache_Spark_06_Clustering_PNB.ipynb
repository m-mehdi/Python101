{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Apache_Spark_06_Clustering.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m-mehdi/Python101/blob/master/Apache_Spark_06_Clustering_PNB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-nugVqFgyKu"
      },
      "source": [
        "<img src=\"images/cads-logo.png\" style=\"height: 100px;\" align=left> <img src=\"images/apache_spark.png\" style=\"height: 20%;width:20%\" align=right>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wETDggrggyKv"
      },
      "source": [
        "# Clustering\n",
        "In clustering, we are going to see if there are natural grouping among the data. So, for example, let's take a look at the utilization data, and see if we can divide this data set into three groups that logically come together. So to do that, we need the Apache Spark Machine Learning package. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_EcRydEgyKv",
        "outputId": "0097179e-5d34-4984-d1cd-3816baa694e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "!pip install pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.clustering import KMeans"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n",
            "\u001b[K     |████████████████████████████████| 204.2MB 62kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 34.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=06b44c487f9aafb715ff9c45eaca6184952e8692145ada6774fb1a1ff4264f43\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ti5C6g5vgyK0"
      },
      "source": [
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEi-yfa2gyK3"
      },
      "source": [
        "import os\n",
        "MAIN_DIRECTORY = os.getcwd()\n",
        "file_path =MAIN_DIRECTORY+\"/Data/utilization.json\"\n",
        "df_util = spark.read.format(\"json\").load(file_path)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwGXB0QugyK7",
        "outputId": "7663ab09-dd7e-4aa3-d14f-496077eeed53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "df_util.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+-------------------+-----------+---------+-------------+\n",
            "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
            "+---------------+-------------------+-----------+---------+-------------+\n",
            "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n",
            "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|\n",
            "|           0.56|03/05/2019 08:16:14|       0.57|      100|           62|\n",
            "|           0.57|03/05/2019 08:21:14|       0.56|      100|           50|\n",
            "|           0.35|03/05/2019 08:26:14|       0.46|      100|           43|\n",
            "|           0.41|03/05/2019 08:31:14|       0.58|      100|           48|\n",
            "|           0.57|03/05/2019 08:36:14|       0.35|      100|           58|\n",
            "|           0.41|03/05/2019 08:41:14|        0.4|      100|           58|\n",
            "|           0.53|03/05/2019 08:46:14|       0.35|      100|           62|\n",
            "|           0.51|03/05/2019 08:51:14|        0.6|      100|           45|\n",
            "|           0.32|03/05/2019 08:56:14|       0.37|      100|           47|\n",
            "|           0.62|03/05/2019 09:01:14|       0.59|      100|           60|\n",
            "|           0.66|03/05/2019 09:06:14|       0.72|      100|           57|\n",
            "|           0.54|03/05/2019 09:11:14|       0.54|      100|           44|\n",
            "|           0.29|03/05/2019 09:16:14|        0.4|      100|           47|\n",
            "|           0.43|03/05/2019 09:21:14|       0.68|      100|           66|\n",
            "|           0.49|03/05/2019 09:26:14|       0.66|      100|           65|\n",
            "|           0.64|03/05/2019 09:31:14|       0.55|      100|           66|\n",
            "|           0.42|03/05/2019 09:36:14|        0.6|      100|           42|\n",
            "|           0.55|03/05/2019 09:41:14|       0.59|      100|           63|\n",
            "+---------------+-------------------+-----------+---------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTe_n28sgyK-"
      },
      "source": [
        "Now, we would like to group data based on the CPU utilization, free memory, and session count. Spark MLLib works with something called a vector. A vector is basically like an array or single data structure that holds all the values from a particular row that the ML algorithm will be looking at. So in our case, we are going to look at only three columns, `cpu_utilization`, `free_memory`, and `session_count`.\n",
        "\n",
        "Now, we are going to create a vector to store these three values, and we do that by calling `VectorAssembler`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZg9khU9gyK-"
      },
      "source": [
        "vecAssembler = VectorAssembler(inputCols=['cpu_utilization','free_memory','session_count'], outputCol='features')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yf-LDnTxgyLB"
      },
      "source": [
        "Now, VectorAssembler returns a data structure, and then we will use this data structure to create a DataFrame by combining the mentioned columns into a single vector and put that vector in a new column called `features`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KQupENBgyLB"
      },
      "source": [
        "vecUtil_df = vecAssembler.transform(df_util)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzwgNV8fgyLE",
        "outputId": "eeb7004e-e855-4b40-97f0-28b48e85154f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "vecUtil_df.show(10)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+-------------------+-----------+---------+-------------+----------------+\n",
            "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|        features|\n",
            "+---------------+-------------------+-----------+---------+-------------+----------------+\n",
            "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|[0.57,0.51,47.0]|\n",
            "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|[0.47,0.62,43.0]|\n",
            "|           0.56|03/05/2019 08:16:14|       0.57|      100|           62|[0.56,0.57,62.0]|\n",
            "|           0.57|03/05/2019 08:21:14|       0.56|      100|           50|[0.57,0.56,50.0]|\n",
            "|           0.35|03/05/2019 08:26:14|       0.46|      100|           43|[0.35,0.46,43.0]|\n",
            "|           0.41|03/05/2019 08:31:14|       0.58|      100|           48|[0.41,0.58,48.0]|\n",
            "|           0.57|03/05/2019 08:36:14|       0.35|      100|           58|[0.57,0.35,58.0]|\n",
            "|           0.41|03/05/2019 08:41:14|        0.4|      100|           58| [0.41,0.4,58.0]|\n",
            "|           0.53|03/05/2019 08:46:14|       0.35|      100|           62|[0.53,0.35,62.0]|\n",
            "|           0.51|03/05/2019 08:51:14|        0.6|      100|           45| [0.51,0.6,45.0]|\n",
            "+---------------+-------------------+-----------+---------+-------------+----------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGdPbdrdgyLH"
      },
      "source": [
        "Now, we want to use this DataFrame in our clustering algorithm, all combined into a single column called `features`. The reason we did this is because the Machine Learning algorithms in Spark MLLib expect the input data to be in a single vector. And now the ML algorithm, we are going to use is called **KMeans**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMDZaRm2gyLH"
      },
      "source": [
        "# setK(3) number of clusters\n",
        "# setSeed(1) it takes a seed for random value generation\n",
        "kmeans = KMeans().setK(3).setSeed(1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-M3cLjaegyLJ"
      },
      "source": [
        "Now, `kmeans` is a data structure that is ready to run the KMeans algorithm. To do that, we will use `fit()`, and `fit()` is the command that is used to actually take input data and then apply the algorithm. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opa4DoZpgyLK"
      },
      "source": [
        "kmModel=kmeans.fit(vecUtil_df)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsHF63iwgyLM"
      },
      "source": [
        "The critical thing in a KMeans model is the cluster centers or centroids. So let's look up what the centroids are."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNWXWyrtgyLN",
        "outputId": "5f4b78a5-fb09-4bdd-b4e5-1c3d6cfe28d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "kmModel.clusterCenters()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([ 0.46901386,  0.52938041, 48.2003027 ]),\n",
              " array([ 0.75054326,  0.24974553, 90.25809685]),\n",
              " array([ 0.64528988,  0.35404374, 70.1378809 ])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBV58-TVi3zO"
      },
      "source": [
        "from pyspark.ml.evaluation import ClusteringEvaluator"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0zTOd--i_Na"
      },
      "source": [
        "pred_cluster_df = kmModel.transform(vecUtil_df)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw0o1jYQi_Py",
        "outputId": "2b658679-614e-48df-83d9-dbb48d73c65d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "pred_cluster_df.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+-------------------+-----------+---------+-------------+----------------+----------+\n",
            "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|        features|prediction|\n",
            "+---------------+-------------------+-----------+---------+-------------+----------------+----------+\n",
            "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|[0.57,0.51,47.0]|         0|\n",
            "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|[0.47,0.62,43.0]|         0|\n",
            "|           0.56|03/05/2019 08:16:14|       0.57|      100|           62|[0.56,0.57,62.0]|         2|\n",
            "|           0.57|03/05/2019 08:21:14|       0.56|      100|           50|[0.57,0.56,50.0]|         0|\n",
            "|           0.35|03/05/2019 08:26:14|       0.46|      100|           43|[0.35,0.46,43.0]|         0|\n",
            "|           0.41|03/05/2019 08:31:14|       0.58|      100|           48|[0.41,0.58,48.0]|         0|\n",
            "|           0.57|03/05/2019 08:36:14|       0.35|      100|           58|[0.57,0.35,58.0]|         0|\n",
            "|           0.41|03/05/2019 08:41:14|        0.4|      100|           58| [0.41,0.4,58.0]|         0|\n",
            "|           0.53|03/05/2019 08:46:14|       0.35|      100|           62|[0.53,0.35,62.0]|         2|\n",
            "|           0.51|03/05/2019 08:51:14|        0.6|      100|           45| [0.51,0.6,45.0]|         0|\n",
            "|           0.32|03/05/2019 08:56:14|       0.37|      100|           47|[0.32,0.37,47.0]|         0|\n",
            "|           0.62|03/05/2019 09:01:14|       0.59|      100|           60|[0.62,0.59,60.0]|         2|\n",
            "|           0.66|03/05/2019 09:06:14|       0.72|      100|           57|[0.66,0.72,57.0]|         0|\n",
            "|           0.54|03/05/2019 09:11:14|       0.54|      100|           44|[0.54,0.54,44.0]|         0|\n",
            "|           0.29|03/05/2019 09:16:14|        0.4|      100|           47| [0.29,0.4,47.0]|         0|\n",
            "|           0.43|03/05/2019 09:21:14|       0.68|      100|           66|[0.43,0.68,66.0]|         2|\n",
            "|           0.49|03/05/2019 09:26:14|       0.66|      100|           65|[0.49,0.66,65.0]|         2|\n",
            "|           0.64|03/05/2019 09:31:14|       0.55|      100|           66|[0.64,0.55,66.0]|         2|\n",
            "|           0.42|03/05/2019 09:36:14|        0.6|      100|           42| [0.42,0.6,42.0]|         0|\n",
            "|           0.55|03/05/2019 09:41:14|       0.59|      100|           63|[0.55,0.59,63.0]|         2|\n",
            "+---------------+-------------------+-----------+---------+-------------+----------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jl4tXzy2i_Kd"
      },
      "source": [
        "eval = ClusteringEvaluator()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr6C70XdjRP2"
      },
      "source": [
        "SED_value = eval.evaluate(pred_cluster_df)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3NjfqkQjRNP",
        "outputId": "ba3dff8c-9910-4950-86d7-0fb5b1587514",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Squared Euclidean Distance Value = {:.2f}\".format(SED_value))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Squared Euclidean Distance Value = 0.74\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swK__HZsgyLP"
      },
      "source": [
        "#### Well Done!"
      ]
    }
  ]
}